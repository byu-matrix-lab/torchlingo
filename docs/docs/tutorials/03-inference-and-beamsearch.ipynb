{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2211d236",
   "metadata": {},
   "source": [
    "# Tutorial 3: Inference and Beam Search\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/commanderjcc/torchlingo/blob/main/docs/docs/tutorials/03-inference-and-beamsearch.ipynb)\n",
    "\n",
    "Generate translations using greedy and beam search decoding strategies.\n",
    "\n",
    "**⚠️ Prerequisites:** This tutorial requires the model checkpoint from Tutorial 2. Run Tutorial 2 first!\n",
    "\n",
    "**⚡ Running in Google Colab?** Make sure to:\n",
    "1. Go to **Runtime → Change runtime type → GPU** (optional)\n",
    "2. Uncomment and run the `%pip install torchlingo` cell below\n",
    "3. Run Tutorial 2 first to create the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TorchLingo (uncomment in Google Colab)\n",
    "# %pip install torchlingo\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c24437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (will use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import TorchLingo modules\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from torchlingo.models import SimpleTransformer\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from Tutorial 2\n",
    "ckpt_path = Path(\"checkpoints/tiny_model.pt\")\n",
    "\n",
    "if not ckpt_path.exists():\n",
    "    print(\"⚠️ No checkpoint found. Please run Tutorial 2 first!\")\n",
    "else:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    model = SimpleTransformer(\n",
    "        src_vocab_size=len(checkpoint['src_vocab']),\n",
    "        tgt_vocab_size=len(checkpoint['tgt_vocab']),\n",
    "        **checkpoint['config'],\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    src_vocab = checkpoint['src_vocab']\n",
    "    tgt_vocab = checkpoint['tgt_vocab']\n",
    "    \n",
    "    print(f\"✓ Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98209da9",
   "metadata": {},
   "source": [
    "## Part 1: Greedy Decoding (Review)\n",
    "\n",
    "Greedy decoding picks the most likely token at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a24f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_sentence, src_vocab, tgt_vocab, device, max_len=20):\n",
    "    \"\"\"Generate translation using greedy decoding.\n",
    "    \n",
    "    At each step, pick the single most likely next token.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_indices = src_vocab.encode(src_sentence, add_special_tokens=True)\n",
    "    src_tensor = torch.tensor([src_indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_tensor)\n",
    "    \n",
    "    # Decode\n",
    "    output_indices = [tgt_vocab.sos_idx]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor([output_indices]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model.decode(tgt_tensor, memory)\n",
    "        \n",
    "        # Greedy: pick argmax\n",
    "        next_token = logits[0, -1, :].argmax().item()\n",
    "        output_indices.append(next_token)\n",
    "        \n",
    "        if next_token == tgt_vocab.eos_idx:\n",
    "            break\n",
    "    \n",
    "    return tgt_vocab.decode(output_indices, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test greedy decoding\n",
    "test_sentence = \"Hello world\"\n",
    "translation = greedy_decode(model, test_sentence, src_vocab, tgt_vocab, device)\n",
    "print(f\"Greedy: '{test_sentence}' → '{translation}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e22ec",
   "metadata": {},
   "source": [
    "### Greedy Limitations\n",
    "\n",
    "Greedy decoding can get stuck in suboptimal paths:\n",
    "\n",
    "```\n",
    "Step 1: P(\"El\") = 0.4, P(\"La\") = 0.35, P(\"Un\") = 0.25\n",
    "        → Pick \"El\" (highest)\n",
    "        \n",
    "Step 2: P(\"gato\"|\"El\") = 0.3, P(\"perro\"|\"El\") = 0.25, ...\n",
    "        → But maybe \"La casa\" would have been better overall!\n",
    "```\n",
    "\n",
    "Greedy only considers one path—it can't backtrack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d51e57",
   "metadata": {},
   "source": [
    "## Part 2: Beam Search\n",
    "\n",
    "Beam search keeps track of multiple hypotheses (\"beams\") and picks the best complete sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(\n",
    "    model, src_sentence, src_vocab, tgt_vocab, device,\n",
    "    beam_size=3, max_len=20, length_penalty=0.6\n",
    "):\n",
    "    \"\"\"Generate translation using beam search.\n",
    "    \n",
    "    Keeps beam_size hypotheses at each step and returns the best one.\n",
    "    \n",
    "    Args:\n",
    "        beam_size: Number of hypotheses to keep\n",
    "        length_penalty: Penalize/reward longer sequences (alpha in paper)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_indices = src_vocab.encode(src_sentence, add_special_tokens=True)\n",
    "    src_tensor = torch.tensor([src_indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_tensor)\n",
    "    \n",
    "    # Initialize beams: (sequence, log_prob)\n",
    "    beams = [([tgt_vocab.sos_idx], 0.0)]\n",
    "    completed = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            # Skip completed sequences\n",
    "            if seq[-1] == tgt_vocab.eos_idx:\n",
    "                completed.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Get probabilities for next token\n",
    "            tgt_tensor = torch.tensor([seq]).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model.decode(tgt_tensor, memory)\n",
    "            \n",
    "            log_probs = F.log_softmax(logits[0, -1, :], dim=-1)\n",
    "            \n",
    "            # Get top beam_size candidates\n",
    "            topk_log_probs, topk_indices = log_probs.topk(beam_size)\n",
    "            \n",
    "            for log_prob, idx in zip(topk_log_probs, topk_indices):\n",
    "                new_seq = seq + [idx.item()]\n",
    "                new_score = score + log_prob.item()\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "        \n",
    "        # Keep top beam_size candidates\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = all_candidates[:beam_size]\n",
    "        \n",
    "        # Stop if all beams are completed\n",
    "        if not beams:\n",
    "            break\n",
    "    \n",
    "    # Add any remaining beams to completed\n",
    "    completed.extend(beams)\n",
    "    \n",
    "    # Apply length penalty and pick best\n",
    "    def score_with_length_penalty(seq, score):\n",
    "        length = len(seq)\n",
    "        return score / (length ** length_penalty)\n",
    "    \n",
    "    best_seq, best_score = max(\n",
    "        completed, \n",
    "        key=lambda x: score_with_length_penalty(x[0], x[1])\n",
    "    )\n",
    "    \n",
    "    return tgt_vocab.decode(best_seq, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7253eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test beam search\n",
    "test_sentence = \"Hello world\"\n",
    "\n",
    "greedy_result = greedy_decode(model, test_sentence, src_vocab, tgt_vocab, device)\n",
    "beam_result = beam_search_decode(model, test_sentence, src_vocab, tgt_vocab, device, beam_size=3)\n",
    "\n",
    "print(f\"Input:  '{test_sentence}'\")\n",
    "print(f\"Greedy: '{greedy_result}'\")\n",
    "print(f\"Beam-3: '{beam_result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0c1c6",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71513f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on multiple sentences\n",
    "test_sentences = [\n",
    "    \"Hello world\",\n",
    "    \"Good morning\",\n",
    "    \"Thank you\",\n",
    "    \"I love you\",\n",
    "    \"The cat sleeps\",\n",
    "]\n",
    "\n",
    "print(f\"{'Input':<20} {'Greedy':<20} {'Beam-3':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for src in test_sentences:\n",
    "    greedy = greedy_decode(model, src, src_vocab, tgt_vocab, device)\n",
    "    beam = beam_search_decode(model, src, src_vocab, tgt_vocab, device)\n",
    "    print(f\"{src:<20} {greedy:<20} {beam:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0856072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of beam size\n",
    "test_sentence = \"Hello world\"\n",
    "\n",
    "print(f\"Input: '{test_sentence}'\")\n",
    "print(\"-\" * 40)\n",
    "for beam_size in [1, 2, 3, 5, 10]:\n",
    "    result = beam_search_decode(\n",
    "        model, test_sentence, src_vocab, tgt_vocab, device,\n",
    "        beam_size=beam_size\n",
    "    )\n",
    "    print(f\"Beam-{beam_size:2d}: '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7ed6a",
   "metadata": {},
   "source": [
    "## Part 4: BLEU Score Evaluation\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) measures translation quality by comparing n-gram overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sacrebleu if needed\n",
    "try:\n",
    "    from sacrebleu.metrics import BLEU\n",
    "    print(\"sacrebleu is installed!\")\n",
    "except ImportError:\n",
    "    print(\"Installing sacrebleu...\")\n",
    "    !pip install sacrebleu\n",
    "    from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ccbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "# Our test data\n",
    "sources = [\n",
    "    \"Hello world\",\n",
    "    \"Good morning\",\n",
    "    \"Thank you\",\n",
    "    \"I love you\",\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"Hola mundo\",\n",
    "    \"Buenos días\",\n",
    "    \"Gracias\",\n",
    "    \"Te amo\",\n",
    "]\n",
    "\n",
    "# Generate translations\n",
    "greedy_translations = [greedy_decode(model, s, src_vocab, tgt_vocab, device) for s in sources]\n",
    "beam_translations = [beam_search_decode(model, s, src_vocab, tgt_vocab, device) for s in sources]\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu = BLEU()\n",
    "\n",
    "greedy_bleu = bleu.corpus_score(greedy_translations, [references])\n",
    "beam_bleu = bleu.corpus_score(beam_translations, [references])\n",
    "\n",
    "print(f\"BLEU Scores:\")\n",
    "print(f\"  Greedy: {greedy_bleu.score:.2f}\")\n",
    "print(f\"  Beam-3: {beam_bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison\n",
    "print(f\"{'Source':<20} {'Reference':<20} {'Greedy':<20} {'Beam':<20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for src, ref, greedy, beam in zip(sources, references, greedy_translations, beam_translations):\n",
    "    print(f\"{src:<20} {ref:<20} {greedy:<20} {beam:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a63c02",
   "metadata": {},
   "source": [
    "## Understanding BLEU\n",
    "\n",
    "BLEU measures n-gram precision:\n",
    "\n",
    "| Score | Quality |\n",
    "|-------|------|\n",
    "| < 10 | Almost unusable |\n",
    "| 10-20 | Gist is clear |\n",
    "| 20-30 | Understandable |\n",
    "| 30-40 | Good quality |\n",
    "| 40-50 | High quality |\n",
    "| > 50 | Very high quality |\n",
    "\n",
    "⚠️ Our toy model with tiny data won't achieve great BLEU scores—that's expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd9561",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Greedy decoding**: Fast but can miss better translations\n",
    "2. **Beam search**: Explores multiple paths, often better results\n",
    "3. **Length penalty**: Prevents beam search from preferring short sequences\n",
    "4. **BLEU score**: Standard metric for translation quality\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Beam size 3-5** is usually sufficient (diminishing returns after)\n",
    "- **Length penalty** around 0.6-1.0 works well\n",
    "- **BLEU** is useful but not perfect—humans judge translation differently\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- Explore the [API Reference](../reference/index.md) for more details\n",
    "- Learn about [SentencePiece](../reference/preprocessing/sentencepiece.md) for better tokenization\n",
    "- Try training on a real dataset!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
